# -*- coding: utf-8 -*-
"""MTLR for Survival Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CkIEzctAE8_WPsXQdt2xPkY1kwbN3BSX

# Multi-task logistic regression for individual survival prediction
Multi-task logistic regression (MTLR) is a machine learning algorithm for survival prediction. It overcomes many of the limitations of existing methods (e.g. the Cox proportional hazard model) by directly modelling the survival curve of each individual, allowing for more accurate individualised survival prediction. This notebook contains a PyTorch implementation of MTLR, together with experiments on simulated and real data. In my [blog post](https://mkazmierski.me/ml/2020/12/08/mtlr_survival.html), you can find derivation and theoretical background.
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install lifelines git+https://github.com/mkazmier/torchmtlr.git

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
from math import sqrt, ceil
from collections import defaultdict

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from torch.optim import Adam
from sklearn.model_selection import KFold, train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import brier_score_loss, roc_auc_score
from lifelines import CoxPHFitter, KaplanMeierFitter
from lifelines.utils import concordance_index
from tqdm import tqdm, trange

from torchmtlr import (MTLR, mtlr_neg_log_likelihood,
                       mtlr_survival, mtlr_survival_at_times,
                       mtlr_risk)
from torchmtlr.utils import (make_time_bins, encode_survival,
                             make_optimizer)

torch.manual_seed(42)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
np.random.seed(42)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

sns.set(context="poster", style="white")
plt.rcParams["figure.figsize"] = (10, 7)

"""## MTLR implementation
Quick remark on notation: subscripts (e.g. $x_k$) denote indexing over timepoints, while superscripts (like $y^{(j)}$) indicate indexing over datapoints.

First comes the MTLR likelihood implementation. You can find the original likelihood definition in [the original publication](https://papers.nips.cc/paper/4210-learning-patient-specific-cancer-survival-distributions-as-a-sequence-of-dependent-regressors). Briefly, the approach is to discretise the time axis into $K$ bins and fit a logistic regression model to predict the probability of event in each bin.

<center><img src="https://github.com/mkazmier/torchmtlr/blob/master/notebooks/time_bins.png?raw=1" width="600"> </center>

*[Image source](https://era.library.ualberta.ca/items/3deb4dd9-788d-4c61-94a5-d3ee6645f74f).*


The probability of event time $T$ falling in the bin $[\tau_{i-1}, \tau_i)$ given covariates $\mathbf{x}$ is modelled as:

$$
P(\tau_{i-1} \le T < \tau_i \mid \mathbf{x}, \boldsymbol{\theta}) = \frac{\exp(\sum_{k=i}^{K-1}\boldsymbol{\theta}_k^T \mathbf{x} + b_k)}{Z(\mathbf{x}, \boldsymbol{\Theta})},
$$

where $\boldsymbol{\theta}_k, b_k$ are the model parameters and $Z = \sum_{i=1}^K \exp(\sum_{k=i}^{K-1}\boldsymbol{\theta}_k^T\mathbf{x} + b_k)$ is the normalizing constant.

Here, we use the reformulation as softmax classifier (first derived [here](https://era.library.ualberta.ca/items/3deb4dd9-788d-4c61-94a5-d3ee6645f74f)), allowing for more efficient computation. It essentially boils down to adding additional 'catch-all' time interval $[t_K, \infty)$ and slightly changing the target encoding, which lets us treat the problem as multi-class classification. Each uncensored instance is now assigned a single *class label*, which is represented as one-hot encoded binary vector. For instances censored at time $t_c$, we know that their true class label lies between $c_c$ and $c_K$, which is encoded as a binary vector with 1s at all indices $c-1 \le i \le K-1$. In this case, the 1s indicate the timepoints over which to marginalise. For example, if the time axis is split into 5 bins, an instance experiencing event in bin 3 (0-indexed) is encoded as `[0, 0, 0, 1, 0]`, and instance censored in bin 2 as `[0, 0, 1, 1, 1]`.

The log-likelihood for uncensored instances is similar to the standard softmax log-likelihood:

$$
L(\boldsymbol{\Theta}, \mathbf{x}, \mathbf{y}) = \sum_{k=i}^{K-1}(\boldsymbol{\theta}_k^T\mathbf{x} + b_k) - \log(Z(\mathbf{x}, \boldsymbol{\Theta})),
$$

where $i$ is the index of non-zero entry in $\mathbf{y}$.

For censored instances, we need to sum (marginalize) over all possible class probabilities:

$$
L(\boldsymbol{\Theta}, \mathbf{x}, \mathbf{y}) = \log(\sum_i \exp(\sum_{k=i}^{K-1}\boldsymbol{\theta}_k^T\mathbf{x} + b_k) - \log(Z(\mathbf{x}, \boldsymbol{\Theta})),
$$

where the sum is over all non-zero entries in $\mathbf{y}$. Here, the summation is over all timepoints later than the censoring time $T_c$.

The full log-likelihood for a dataset $D =
\{T^{(j)}, \delta^{(j)}, \mathbf{x}^{(j)}\}_{j=1}^{N}$ with $N$ instances, $N_c$ of which are censored, is

$$
L(\boldsymbol{\Theta}, D) =
\sum_{j=1}^{N-N_c}\sum_{k=1}^{K-1}(\boldsymbol{\theta}_k^T\mathbf{x}^{(j)} + b_k)\mathbf{y}_k^{(j)} +
\sum_{j=N_c}^{N}\log(\sum_{t_k > T_c^{(j)}} \exp(\sum_{k=1}^{K-1}\boldsymbol{\theta}_k^T\mathbf{x}^{(j)} + b_k)
- \sum_{j=1}^{N}\log(Z(\mathbf{x}^{(j)}, \boldsymbol{\Theta})) + \frac{C_1}{2}\sum_{k=1}^{K-1}\Vert\boldsymbol{\theta}_k\Vert^2.
$$

$C_1$ is a hyperparameter determining the strength of $\ell_2$ regularization, which also controls the smoothness of predicted survival curves..
"""

def normalize(data, mean=None, std=None, skip_cols=[]):
    """Normalizes the columns of Pandas DataFrame to zero mean and unit
    standard deviation."""
    if mean is None:
        mean = data.mean(axis=0)
    if std is None:
        std = data.std(axis=0)
    if skip_cols is not None:
        mean[skip_cols] = 0
        std[skip_cols] = 1
    return (data - mean) / std, mean, std


def reset_parameters(model):
    """Resets the parameters of a PyTorch module and its children."""
    for m in model.modules():
        try:
            m.reset_parameters()
        except AttributeError:
            continue
    return model


# training functions
def train_mtlr(model, data_train, time_bins,
               num_epochs=1000, lr=.01, weight_decay=0.,
               C1=1., batch_size=None,
               verbose=True, device="cpu"):
    """Trains the MTLR model using minibatch gradient descent.

    Parameters
    ----------
    model : torch.nn.Module
        MTLR model to train.
    data_train : pd.DataFrame
        The training dataset. Must contain a `time` column with the
        event time for each sample and an `event` column containing
        the event indicator.
    num_epochs : int
        Number of training epochs.
    lr : float
        The learning rate.
    weight_decay : float
        Weight decay strength for all parameters *except* the MTLR
        weights. Only used for Deep MTLR training.
    C1 : float
        L2 regularization (weight decay) strenght for MTLR parameters.
    batch_size : int
        The batch size.
    verbose : bool
        Whether to display training progress.
    device : str
        Device name or ID to use for training.

    Returns
    -------
    torch.nn.Module
        The trained model.
    """
    x = torch.tensor(data_train.drop(["time", "event"], axis=1).values, dtype=torch.float)
    y = encode_survival(data_train["time"].values, data_train["event"].values, time_bins)
    optimizer = make_optimizer(Adam, model, lr=lr, weight_decay=weight_decay)
    reset_parameters(model)
    model = model.to(device)
    model.train()
    train_loader = DataLoader(TensorDataset(x, y), batch_size=batch_size, shuffle=True)

    pbar =  trange(num_epochs, disable=not verbose)
    for i in pbar:
        for xi, yi in train_loader:
            xi, yi = xi.to(device), yi.to(device)
            y_pred = model(xi)
            loss = mtlr_neg_log_likelihood(y_pred, yi, model, C1=C1, average=True)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        pbar.set_description(f"[epoch {i+1: 4}/{num_epochs}]")
        pbar.set_postfix_str(f"loss = {loss.item():.4f}")
    model.eval()
    return model


def train_mtlr_cv(model, data_train, time_bins, cv=3, C1_vals=None, verbose=True, device="cpu", **kwargs):
    """Trains the MTLR model using minibatch gradient descent,
    determining the optimal L2 regularization strength with K-fold
    cross-validation.

    Parameters
    ----------
    model : torch.nn.Module
        MTLR model to train.
    data_train : pd.DataFrame
        The training dataset. Must contain a `time` column with the
        event time for each sample and an `event` column containing
        the event indicator.
    cv : int
        The number of cross-validation splits to use.
    C1_vals : float, optional
        L2 regularization (weight decay) values to test. If None (default),
        search over `np.logspace(-2, 3, 6)`.
    verbose : bool
        Whether to display training progress.
    device : str
        Device name or ID to use for training.
    kwargs
        Additional keyword arguments passed to `train_mtlr`.

    Returns
    -------
    torch.nn.Module
        The trained model.
    """
    if C1_vals is None:
        C1_vals = np.logspace(-2, 3, 6)

    kfold = KFold(n_splits=cv)
    nll_vals = defaultdict(list)
    for C1 in C1_vals:
        pbar = tqdm(kfold.split(data_train),
                    total=cv,
                    disable=not verbose,
                    desc=f"testing C1 = {C1:9}")
        for train_idx, val_idx in pbar:
            train_fold, val_fold = data_train.iloc[train_idx], data_train.iloc[val_idx]
            time_val, event_val = data_train.iloc[val_idx]["time"].values, data_train.iloc[val_idx]["event"].values
            x_val = torch.tensor(val_fold.drop(["time", "event"], axis=1).values,
                                 dtype=torch.float, device=device)
            y_val = encode_survival(time_val, event_val, time_bins).to(device)
            model = train_mtlr(model, train_fold, time_bins, C1=C1, device=device, verbose=False, **kwargs)
            with torch.no_grad():
                val_nll = mtlr_neg_log_likelihood(model(x_val), y_val, model, C1=C1)
                nll_vals[C1].append(val_nll.item())
            pbar.set_postfix_str(f"val nll = {val_nll.item():.2f}")

    # Choose regularization parameter with the lowest negative log-likelihood
    best_C1 = min(nll_vals, key=lambda k: sum(nll_vals[k]) / cv)

    if verbose:
        print(f"training with C1 = {best_C1}")

    model = train_mtlr(model, data_train, time_bins, C1=best_C1, device=device, verbose=verbose, **kwargs)
    return model

"""## Experiments with simulated data

Let's first see how MTLR performs on a simulated dataset. We'll generate a dataset with 10 Gaussian features and linear, constant hazard. The hazard is given by:

$$
h(\mathbf{x}) = x_1 + 2x_2
$$

and the survival times are generated randomly from an exponential distribution:

$$
T \sim \mathrm{Exp}(\lambda_{\mathrm{base}}\exp(h(\mathbf{x})).
$$

Note that only 2 of the variables correlate with the survival time and the rest are noise. We also adjust the censoring time such that approximately 10% of the observations are censored.
"""

def make_synthetic_data(n_samples=8000, n_noise_features=8, base_hazard=.1, percent_censor=.1):
    """Generates a synthetic survival dataset with linear hazard."""
    x = np.random.standard_normal((n_samples, n_noise_features + 2))
    hazards = x[:, 0] + 2 * x[:, 1]
    event_time = np.random.exponential(1 / (base_hazard * np.exp(hazards)))
    censor_time = np.quantile(event_time, 1 - percent_censor)

    time = np.minimum(event_time, censor_time)
    event = (event_time < censor_time).astype(np.int)

    return pd.DataFrame({
        "time": time,
        "event": event,
        **{f"x{i+1}": x[:, i] for i in range(x.shape[1])}
    })

data = make_synthetic_data()

# prepare data
data_train, data_test = train_test_split(data, test_size=.25, random_state=42)
time_bins = make_time_bins(data_train["time"].values, event=data_train["event"].values)
num_time_bins = len(time_bins)

# fit MTLR model
mtlr = MTLR(in_features=10, num_time_bins=num_time_bins)
mtlr = train_mtlr(mtlr, data_train, time_bins, num_epochs=100,
                  lr=.005, batch_size=512, verbose=True, device=device)

with torch.no_grad():
    pred = mtlr(torch.tensor(data_test.drop(["time", "event"], axis=1).values, dtype=torch.float, device=device))
    pred_survival = mtlr_survival(pred).cpu().numpy()
    pred_risk = mtlr_risk(pred).cpu().numpy()

"""We can check the model's [concordance index](https://stats.stackexchange.com/questions/29815/how-to-interpret-the-output-for-calculating-concordance-index-c-index) to make sure it has converged."""

# Note that the CI function expects the predictions to be higher for
# longer survivors, which can be achieved by flipping the sign of the risk.
ci = concordance_index(data_test["time"], -pred_risk, event_observed=data_test["event"])
print(f"CI = {ci:.2f}")

"""Since the MTLR model associates a parameter with each time bin, we can plot the trained parameter values over time to see how the importance of each feature changes over time."""

def plot_weights(weights, time_bins, feature_names):
    """Plot MTLR model parameters over time."""
    top_idxs = torch.argsort(weights.abs().sum(1), descending=True)[:5]
    fig, ax = plt.subplots()
    for i in top_idxs:
        ax.plot(np.pad(time_bins, (1, 0))[:-1], weights[i], label=feature_names[i])
    ax.set_xlabel("Time (days)")
    ax.set_ylabel("Weight")
    ax.axhline(0, c="k", linestyle="--")
    fig.legend()
    return ax

feature_names = data.drop(["time", "event"], axis=1).columns
plot_weights(mtlr.mtlr_weight.detach().cpu(), time_bins, feature_names)

"""From the plot, we can see that the model is able to correctly identify the 2 important variables ($x_1$ and $x_2$) while assigning low weights to the noise variables. Furthermore, the weights are more or less constant over time (reflecting the constant hazard) and the weight for $x_2$ is approximately twice the weight of $x_1$, as it should be. This gives us reasonable confidence that our MTLR implementation is correct!

## SUPPORT dataset
Now let's test the MTLR model on some real-world data. We'll use the SUPPORT2 dataset (available [here](https://biostat.app.vumc.org/wiki/Main/SupportDesc)), consisting of various physiological and demographic features of about 9000 patients admitted to the ICU and followed up for several months afterwards. The goal is to predict the time to death from any cause. About 30% of the survival times are censored.

We will compare it to the [Cox proportional hazards model](https://en.wikipedia.org/wiki/Proportional_hazards_model), which is a standard approach in survival analysis.
"""

def make_support_data():
    """Downloads and preprocesses the SUPPORT dataset from [1]_.

    The missing values are filled using either the recommended
    standard values, the mean (for continuous variables) or the mode
    (for categorical variables).
    Refer to the dataset description at
    https://biostat.app.vumc.org/wiki/Main/SupportDesc for more information.

    Returns
    -------
    pd.DataFrame
        DataFrame with processed covariates for one patient in each row.

    References
    ----------
    ..[1] W. A. Knaus et al., ‘The SUPPORT Prognostic Model: Objective Estimates of Survival
    for Seriously Ill Hospitalized Adults’, Ann Intern Med, vol. 122, no. 3, p. 191, Feb. 1995.
    """
    url = "https://biostat.app.vumc.org/wiki/pub/Main/DataSets/support2csv.zip"

    # Remove other target columns and other model predictions
    cols_to_drop = [
        "hospdead",
        "slos",
        "charges",
        "totcst",
        "totmcst",
        "avtisst",
        "sfdm2",
        "adlp",
        "adls",
        "dzgroup",
        "sps",
        "aps",
        "surv2m",
        "surv6m",
        "prg2m",
        "prg6m",
        "dnr",
        "dnrday",
        "hday",
    ]

    # `death` is the overall survival event indicator
    # `d.time` is the time to death from any cause or censoring
    data = (pd.read_csv(url)
            .drop(cols_to_drop, axis=1)
            .rename(columns={"d.time": "time", "death": "event"}))
    data["event"] = data["event"].astype(np.int)

    data["ca"] = (data["ca"] == "metastatic").astype(int)

    # use recommended default values from official dataset description ()
    # or mean (for continuous variables)/mode (for categorical variables) if not given
    fill_vals = {
        "alb":     3.5,
        "pafi":    333.3,
        "bili":    1.01,
        "crea":    1.01,
        "bun":     6.51,
        "wblc":    9,
        "urine":   2502,
        "edu":     data["edu"].mean(),
        "ph":      data["ph"].mean(),
        "glucose": data["glucose"].mean(),
        "scoma":   data["scoma"].mean(),
        "meanbp":  data["meanbp"].mean(),
        "hrt":     data["hrt"].mean(),
        "resp":    data["resp"].mean(),
        "temp":    data["temp"].mean(),
        "sod":     data["sod"].mean(),
        "income":  data["income"].mode()[0],
        "race":    data["race"].mode()[0],
    }
    data = data.fillna(fill_vals)

    # one-hot encode categorical variables
    onehot_cols = ["sex", "dzclass", "income", "race"]
    data = pd.get_dummies(data, columns=onehot_cols, drop_first=True)

    return data

data = make_support_data()
data_train, data_test = train_test_split(data, test_size=.3, random_state=32)

percent_censored = 1 - data["event"].mean()
round(percent_censored, 2)

len(data)

"""We'll use 2 metrics to assess the performance of each model:
- [Area under the ROC curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve) at time $t$: measures binary classification performance (alive vs dead) at time $t$. Perfect model would achieve $AUC = 1$, while random guessing scores .5.
- [Brier score](https://en.wikipedia.org/wiki/Brier_score) (aka mean squared error) at time $t$: measures the accuracy and calibration of predicted survival probabilites at time $t$ (lower is better). A perfectly accurate and calibrated model would achieve $\mathrm{BS} = 0$, while a randomly-guessing model (always predicting $P(\mathrm{event}) = .5$) would achieve $\mathrm{BS} = .25$ (since $(1 - .5)^2 = (0 - .5)^2 = .25$).

Following the original publication, we will compute the metrics at 25%, 50% and 75% quantiles of event times.
"""

def compute_metric_at_times(metric, time_true, prob_pred, event_observed, score_times):
    """Helper function to evaluate a metric at given timepoints."""
    scores = []
    for time, pred in zip(score_times, prob_pred.T):
        target = time_true > time
        uncensored = target | event_observed.astype(bool)
        scores.append(metric(target[uncensored], pred[uncensored]))

    return scores


def brier_score_at_times(time_true, prob_pred, event_observed, score_times):
    scores = compute_metric_at_times(brier_score_loss,
                                     time_true,
                                     prob_pred,
                                     event_observed,
                                     score_times)
    return scores


def roc_auc_at_times(time_true, prob_pred, event_observed, score_times):
    scores = compute_metric_at_times(roc_auc_score,
                                     time_true,
                                     prob_pred,
                                     event_observed,
                                     score_times)
    return scores

# evaluate model performance at 25%, 50% and 75% of survival time distribution
eval_times = np.quantile(data.loc[data["event"] == 1, "time"], [.25, .5, .75]).astype(np.int)
metrics = []

data_train, mean_train, std_train = normalize(data_train, skip_cols=["time", "event"])
data_test, *_ = normalize(data_test, mean=mean_train, std=std_train, skip_cols=["time", "event"])
x_test = torch.tensor(data_test.drop(["time", "event"], axis=1).values, dtype=torch.float, device=device)
num_features = data_train.shape[1] - 2
time_bins = make_time_bins(data_train["time"].values, event=data_train["event"].values)
num_time_bins = len(time_bins)

# fit Cox model
cph = CoxPHFitter()
cph.fit(data_train, duration_col="time", event_col="event")

pred_survival = cph.predict_survival_function(data_test, eval_times).values.T

bs = brier_score_at_times(data_test["time"], pred_survival, data_test["event"], eval_times)
auc = roc_auc_at_times(data_test["time"], pred_survival, data_test["event"], eval_times)
metrics.append({
    "model": "cox",
    **{f"bs_{t}": bs[i] for i, t in enumerate(eval_times)},
    **{f"auc_{t}": auc[i] for i, t in enumerate(eval_times)}
})

# fit MTLR model
mtlr = MTLR(in_features=num_features, num_time_bins=num_time_bins)
mtlr = train_mtlr_cv(mtlr, data_train, time_bins, num_epochs=500, lr=.005,
                     cv=3, verbose=True, batch_size=512, device=device)

with torch.no_grad():
    pred = mtlr(x_test)
    pred_survival = mtlr_survival_at_times(pred, time_bins, eval_times)

bs = brier_score_at_times(data_test["time"], pred_survival, data_test["event"], eval_times)
auc = roc_auc_at_times(data_test["time"], pred_survival, data_test["event"], eval_times)
metrics.append({
    "model": "mtlr",
    **{f"bs_{t}": bs[i] for i, t in enumerate(eval_times)},
    **{f"auc_{t}": auc[i] for i, t in enumerate(eval_times)}
})

"""### Performance comparison with Cox model
Because it directly models the survival curve, MTLR is able to achieve better classification AUC than the Cox model (which estimates the survival curve in a post-hoc procedure). MTLR also achieves lower Brier score (especially at early timepoints), indicating better calibration.
"""

pd.DataFrame(metrics).round(3)

"""### Visualizing the predicted survival curves
Let's plot the predicted survival curves for both models for a few randomly chosen subjects. There are a few things to notice here:
- The Cox model generates curves with similar overall shape due to the proportional hazards assumption. Note that the curves for any two patients never cross (unless they have exactly the same features).
- In contrast, the MTLR curves are not constrained to a particular shape. This allows for greater flexibility while still keeping the predictions reasonably smooth.
- The $\ell_2$ regularization strength controls the smoothness of MTLR predicted curves. By default it's selected using cross validation, which should give fairly smooth curves. Try playing around with it and observe how the shape of the curves changes.
"""

idxs = np.random.choice(np.arange(len(data_test)), 10)

fig, ax = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(19, 7))

pred_survival_cph = cph.predict_survival_function(data_test, np.pad(time_bins, (1, 0))).values.T
pred_survival_mtlr = mtlr_survival(pred).cpu()

for idx in idxs:
    ax[0].plot(np.pad(time_bins, (1, 0)), pred_survival_cph[idx])
    ax[1].plot(np.pad(time_bins, (1, 0)), pred_survival_mtlr[idx])

ax[0].set_ylabel("Survival probability")
ax[1].set_ylabel("")
ax[0].set_title("Cox")
ax[1].set_title("MTLR")
fig.text(0.52,0.01, "Time (days)", ha="center", va="center")
fig.tight_layout()

"""### Visualising model parameters
We can have a look at the model parameters to get an idea of the learned relationships between features and the risk of event over time. Here we select the 5 features with the highest sum of absolute weight values over time. Again, the $\ell_2$ regularization strength determines the smoothness of weights. Notice how the weights vary over time, unlike the Cox model (where the hazard ratio for each covariate is constant over time). Although it can be a useful diagnostic tool, you need to be careful not to over-interpret this plot as it can sometimes be misleading, e.g. some features are highly correlated.
"""

feature_names = data.drop(["time", "event"], axis=1).columns
plot_weights(mtlr.mtlr_weight.detach().cpu(), time_bins, feature_names)

"""## Deep MTLR
A cool aspect of MTLR is that it is differentiable end-to-end. This lets us increase the capacity of the model by using a neural net to learn nonlinear representations end-to-end with survival prediction. Simply stack a recognition network on top of the model and watch the autodiff magic happen!
"""

# The MTLR module can be used just like any other PyTorch layer
mtlr = nn.Sequential(
    nn.Linear(num_features, 100),
    nn.ELU(),
    nn.Dropout(.4),
    nn.Linear(100, 32),
    nn.ELU(),
    nn.Dropout(.4),
    MTLR(in_features=32, num_time_bins=num_time_bins)
)

mtlr = train_mtlr_cv(mtlr, data_train, time_bins, num_epochs=500, lr=.005,
                     batch_size=512, cv=3, weight_decay=1e-5, verbose=True, device=device)

with torch.no_grad():
    pred = mtlr(torch.tensor(data_test.drop(["time", "event"], axis=1).values, dtype=torch.float, device=device))
    pred_survival = mtlr_survival_at_times(pred, time_bins, eval_times)

bs = brier_score_at_times(data_test["time"], pred_survival, data_test["event"], eval_times)
auc = roc_auc_at_times(data_test["time"], pred_survival, data_test["event"], eval_times)
metrics.append({
    "model": "deepmtlr",
    **{f"bs_{t}": bs[i] for i, t in enumerate(eval_times)},
    **{f"auc_{t}": auc[i] for i, t in enumerate(eval_times)}
})

pd.DataFrame(metrics).round(3)

"""Using a neural net, we're able to achieve better classification accuracy and calibration than the linear model. The gains could be even bigger if we had a larger dataset to learn from.

## Visualising learned representations
We can inspect the learned features in the penultimate layer (just before the MTLR head) to see if the model is able to learn meaningful representations. We'll use [UMAP](https://arxiv.org/abs/1802.03426) to embed the learned features in 2 dimensions for visualisation.
"""

from umap import UMAP

uncensored = data_train.loc[data_train["event"] == 1]

with torch.no_grad():
    hidden = mtlr[:-1](torch.tensor(uncensored.drop(["time", "event"], axis=1).values, dtype=torch.float, device=device)).cpu().numpy()

umap = UMAP()
hidden_vis = umap.fit_transform(hidden)

plt.scatter(hidden_vis[:, 0], hidden_vis[:, 1], c=np.log(uncensored["time"]), s=20, cmap="Spectral")
plt.colorbar(label="$\log(\mathrm{time})$")

"""The samples seem to cluster by survival time in the embedding space, which seems to vary smoothly along the embedding dimensions. This means that the model is able to learn meaningful representations for survival prediction.

The potential of MTLR does not end here — since it's differentiable, it can be used like any other neural network layer, for example with an LSTM recognition network to handle time-varying features, or with a CNN stem to learn features directly from medical images (something I'm working on right now), or even something more exotic like a [neural ODE](https://papers.nips.cc/paper/8773-latent-ordinary-differential-equations-for-irregularly-sampled-time-series.pdf).
"""